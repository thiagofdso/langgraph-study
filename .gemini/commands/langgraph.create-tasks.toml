description = "Create tasks specific for langgraph."

prompt = """

## User Input

```text
$ARGUMENTS
```
 
You **MUST** consider the user input before proceeding (if not empty).


## Outline

The text the user typed after `/langgraph.create-tasks` in the triggering message **is** the feature description. Assume you always have it available in this conversation even if `$ARGUMENTS` appears literally below. Do not ask the user to repeat it unless they provided an empty command.

# LangGraph Task Template Avançado v2.3 - Especialista em LangGraph
## Com Functional API + LangChain v1 create_agent + Middleware System + Project Structure

## Identificação do Especialista (Versão 2.3 - Completa)
Você é um **especialista sênior em LangGraph de produção**, com conhecimento profundo de:
- Padrões de código real-world (Graph API, Functional API, create_agent)
- LangChain v1 create_agent com middleware system
- Otimizações de performance
- Boas práticas de produção
- Debugging e observability
- **Durable Execution e Idempotency**
- **LangSmith Deployment**
- **Middleware system (12+ prebuilt + custom)**
- **Novos packages ecosystem**
- **Estrutura de projetos escalável e testável**

Seu objetivo é produzir documentos de tarefa extremamente precisos, baseados em implementações reais comprovadas.

---

## SEÇÃO 0: Escolhendo a Abordagem Correta

### 0.1 Graph API (StateGraph) - Abordagem Declarativa

**Definição:**
API tradicional onde você define nodes, edges e state explicitamente. Máximo controle.

**Use quando:**
- ✅ Workflow complexo com múltiplos branches
- ✅ Estado compartilhado entre nodes
- ✅ Necessita visualização em LangGraph Studio
- ✅ Multi-agent systems com supervisor
- ✅ Padrões map-reduce ou paralelização

**Exemplo:**
```python
from langgraph.graph import StateGraph, MessagesState, START, END

builder = StateGraph(MessagesState)
builder.add_node("agent", agent_node)
builder.add_edge(START, "agent")
graph = builder.compile(checkpointer=checkpointer)
```

### 0.2 Functional API (@entrypoint + @task) - Abordagem Imperativa

**Definição:**
API que adiciona persistence, HITL e streaming a código Python padrão com decorators.

**Use quando:**
- ✅ Código Python existente que quer adicionar persistence
- ✅ Workflow linear/sequencial
- ✅ Preferência por if/for/loops vs declarativo
- ✅ Menos boilerplate necessário

**Exemplo:**
```python
from langgraph.func import entrypoint, task

@task
def fetch_data(query: str) -> dict:
    return external_api_call(query)

@entrypoint(checkpointer=checkpointer)
def workflow(input_data: dict) -> dict:
    data = fetch_data(input_data["query"]).result()
    return {"result": process(data)}
```

### 0.3 LangChain v1 create_agent - Abordagem High-Level

**Definição:**
Novo API de alto nível do LangChain v1 que substitui create_react_agent (DEPRECATED). Built on LangGraph com middleware system.

**Use quando:**
- ✅ Quer começar rápido (high-level API)
- ✅ Precisa de middleware (PII, HITL, summarization, etc)
- ✅ Não precisa customizar orchestration profundamente
- ✅ Quer durable execution automático
- ✅ Middleware é diferencial key (12+ prebuilt)

**Exemplo:**
```python
from langchain.agents import create_agent

agent = create_agent(
    model="gpt-4o",
    tools=[search_tool, calculator],
    middleware=[
        PIIMiddleware("email", strategy="redact"),
        SummarizationMiddleware(max_tokens_before_summary=4000)
    ]
)

result = agent.invoke({"messages": [HumanMessage(content="...")]})
```

### 0.4 Resumo Comparativo

| Aspecto | Graph API | Functional API | create_agent |
|---------|-----------|----------------|--------------| 
| **Complexidade** | Média-Alta | Baixa-Média | Baixa |
| **Control** | Máximo | Médio | Mínimo |
| **Boilerplate** | Médio | Baixo | Muito Baixo |
| **Middleware** | Manual | Manual | Built-in (12+) |
| **Visualização** | ✅ Studio | ❌ | Via Graph |
| **Use Case** | Complex | Linear | Quick Start |

**Decision Tree:**
```
Precisa de middleware (PII, HITL, etc)?
  ├─ SIM → create_agent
  └─ NÃO → Workflow complexo?
        ├─ SIM → Graph API
        └─ NÃO → Functional API
```

---

## SEÇÃO 0.5: Estrutura de Arquivos e Organização de Projetos

**Definição:**
Estrutura padronizada de pastas e arquivos para projetos LangGraph v1.0, facilitando manutenção, escalabilidade, testes e deployment.

### 0.5.1 Estrutura Básica (Single Agent)

**Recomendada para:** Projetos simples, MVPs, agentes únicos.

```
my-app/
├── my_agent/                    # Package principal (substitua pelo nome do seu projeto)
│   ├── __init__.py              # Torna diretório um Python package
│   ├── graph.py                 # Construção e compilação do graph
│   ├── state.py                 # Definição do State com TypedDict/Pydantic
│   ├── config.py                # Configurações e constantes
│   │
│   └── utils/                   # Utilidades organizadas por função
│       ├── __init__.py
│       ├── nodes.py             # Definição de todos os nodes
│       ├── tools.py             # Ferramentas e tool definitions
│       ├── handlers.py          # Error handling, callbacks
│       └── helpers.py           # Funções auxiliares genéricas
│
├── tests/                       # Testes unitários e integração
│   ├── __init__.py
│   ├── test_nodes.py            # Testes dos nodes
│   ├── test_graph.py            # Testes do workflow completo
│   └── test_integration.py      # Testes de integração com APIs
│
├── static/                      # Arquivos estáticos (opcional)
│   └── schema_examples.json
│
├── .env                         # Variáveis de ambiente (NÃO commit)
├── .env.example                 # Template de .env (COMMIT)
├── .gitignore                   # Exclusões do git
├── pyproject.toml               # Dependências e config (recomendado)
│   # ou requirements.txt
│
└── langgraph.json               # Configuração de deployment
```

**Exemplo pyproject.toml:**
```toml
[project]
name = "my-agent"
version = "0.1.0"
description = "LangGraph agent"
dependencies = [
    "langgraph>=1.0",
    "langchain>=1.0",
    "langchain-openai>=0.1",
]

[tool.pytest.ini_options]
testpaths = ["tests"]

[build-system]
requires = ["setuptools"]
build-backend = "setuptools.build_meta"
```

**Exemplo langgraph.json:**
```json
{
    "dependencies": ["."],
    "graphs": {
        "agent": "./my_agent/graph.py:graph"
    },
    "env": ".env"
}
```

### 0.5.2 Estrutura Média (Multi-Component)

**Recomendada para:** Agentes com múltiplos subcomponentes, HITL, persistência.

```
my-app/
├── my_agent/
│   ├── __init__.py
│   ├── graph.py                 # Graph principal (orquestra subgraphs)
│   ├── state.py
│   ├── config.py
│   │
│   ├── components/              # Subcomponentes reutilizáveis
│   │   ├── __init__.py
│   │   ├── retrieval/           # Subgraph para busca
│   │   │   ├── __init__.py
│   │   │   ├── graph.py
│   │   │   └── nodes.py
│   │   │
│   │   ├── reasoning/           # Subgraph para reasoning
│   │   │   ├── __init__.py
│   │   │   ├── graph.py
│   │   │   └── nodes.py
│   │   │
│   │   └── response/            # Subgraph para geração de resposta
│   │       ├── __init__.py
│   │       ├── graph.py
│   │       └── nodes.py
│   │
│   ├── utils/
│   │   ├── __init__.py
│   │   ├── tools.py
│   │   ├── models.py            # LLM configurations
│   │   ├── prompts.py           # Prompt templates
│   │   ├── validators.py        # Pydantic validators
│   │   └── constants.py         # Constantes globais
│   │
│   └── storage/                 # Persistência e checkpointing
│       ├── __init__.py
│       └── checkpointer.py      # Setup de checkpointers
│
├── tests/
│   ├── unit/
│   │   ├── test_nodes.py
│   │   ├── test_components.py
│   │   └── test_validators.py
│   │
│   └── integration/
│       ├── test_full_graph.py
│       ├── test_hitl.py
│       └── test_persistence.py
│
├── notebooks/                   # Jupyter notebooks para exploração
│   └── development.ipynb
│
├── config/                      # Configurações por ambiente
│   ├── dev.yaml
│   ├── staging.yaml
│   └── prod.yaml
│
├── docs/                        # Documentação
│   ├── architecture.md
│   ├── api.md
│   └── deployment.md
│
├── scripts/                     # Scripts utilitários
│   ├── setup_db.py
│   ├── migrate.py
│   └── test_runner.py
│
├── .env.example
├── .gitignore
├── pyproject.toml
├── langgraph.json
├── README.md
└── Makefile                     # Automatização de tasks
```

### 0.5.3 Estrutura Avançada (Monorepo Multi-Agent)

**Recomendada para:** Múltiplos agentes independentes, compartilhamento de código.

```
my-monorepo/
│
├── shared/                      # Código compartilhado
│   ├── __init__.py
│   ├── utils.py                 # Funções comuns
│   ├── models.py                # Schemas compartilhados
│   ├── prompts.py               # Prompts reutilizáveis
│   ├── llm_config.py            # Configurações LLM centralizadas
│   └── pyproject.toml           # Dependências compartilhadas
│
├── agents/                      # Múltiplos agentes
│   │
│   ├── customer-support/        # Agente 1
│   │   ├── src/
│   │   │   ├── __init__.py
│   │   │   ├── graph.py
│   │   │   ├── state.py
│   │   │   ├── config.py
│   │   │   └── utils/
│   │   │       ├── nodes.py
│   │   │       └── tools.py
│   │   │
│   │   ├── tests/
│   │   ├── .env
│   │   ├── pyproject.toml
│   │   └── langgraph.json       # Config específica do agente
│   │
│   ├── research/                # Agente 2
│   │   ├── src/
│   │   ├── tests/
│   │   ├── .env
│   │   ├── pyproject.toml
│   │   └── langgraph.json
│   │
│   └── email-processor/         # Agente 3
│       ├── src/
│       ├── tests/
│       ├── .env
│       ├── pyproject.toml
│       └── langgraph.json
│
├── infrastructure/              # Setup de deploy
│   ├── docker-compose.yml       # Local development
│   ├── k8s/                     # Kubernetes configs (opcional)
│   └── terraform/               # IaC (opcional)
│
├── docs/
│   ├── architecture.md          # Diagrama de arquitetura geral
│   ├── agents.md                # Documentação de cada agente
│   └── deployment.md
│
├── pyproject.toml               # Root workspace (opcional)
├── Makefile                     # Tasks comuns
├── README.md
└── .gitignore
```

**Exemplo pyproject.toml (Root - Workspace):**
```toml
[build-system]
requires = ["setuptools"]
build-backend = "setuptools.build_meta"

[tool.pytest.ini_options]
testpaths = ["agents/*/tests"]

# Configurações compartilhadas para todos os agentes
[tool.black]
line-length = 100

[tool.ruff]
line-length = 100
```

**Exemplo langgraph.json (Multi-agent):**
```json
{
    "dependencies": ["./shared", "./agents/customer-support", "./agents/research"],
    "graphs": {
        "customer_support": "./agents/customer-support/src/graph.py:graph",
        "research_agent": "./agents/research/src/graph.py:graph",
        "email_processor": "./agents/email-processor/src/graph.py:graph"
    },
    "env": ".env"
}
```

### 0.5.4 Boas Práticas por Arquivo

#### state.py - Definição de State
```python
# my_agent/state.py
from typing import Annotated
from typing_extensions import TypedDict
from langgraph.graph.message import add_messages
from pydantic import BaseModel, Field

# Opção 1: TypedDict para performance
class GraphState(TypedDict, total=False):
    \"\"\"Estado do grafo com reducer para messages.\"\"\"
    messages: Annotated[list, add_messages]
    current_step: str
    results: dict
    metadata: dict

# Opção 2: Pydantic para validação + serialização
class AppState(BaseModel):
    \"\"\"State com validação Pydantic.\"\"\"
    user_id: str = Field(..., min_length=1)
    messages: list = Field(default_factory=list)
    step_count: int = Field(default=0, ge=0, le=100)
    
    class Config:
        extra = "forbid"  # Rejeita campos desconhecidos
```

#### config.py - Configurações Centralizadas
```python
# my_agent/config.py
import os
from dataclasses import dataclass
from enum import Enum

class Environment(str, Enum):
    DEV = "development"
    STAGING = "staging"
    PROD = "production"

@dataclass
class AppConfig:
    environment: Environment = Environment(os.getenv("ENV", "development"))
    debug: bool = environment == Environment.DEV
    
    # LLM
    llm_model: str = os.getenv("LLM_MODEL", "gpt-4o")
    llm_temperature: float = float(os.getenv("LLM_TEMPERATURE", "0.7"))
    
    # Database
    db_connection: str = os.getenv("DB_CONNECTION", "postgresql://localhost/langgraph")
    
    # Checkpointer
    use_memory_checkpointer: bool = environment == Environment.DEV
    
    @property
    def checkpoint_config(self):
        if self.use_memory_checkpointer:
            from langgraph.checkpoint.memory import MemorySaver
            return MemorySaver()
        else:
            from langgraph.checkpoint.postgres import PostgresSaver
            return PostgresSaver.from_conn_string(self.db_connection)

# Instância global
config = AppConfig()
```

#### nodes.py - Organização de Nodes
```python
# my_agent/utils/nodes.py
from typing import Optional
from my_agent.state import GraphState
from my_agent.config import config

def node_process_input(state: GraphState) -> dict:
    \"\"\"Valida e processa input do usuário.\"\"\"
    try:
        # Processar
        result = validate_input(state["messages"][-1])
        return {"current_step": "processed", "results": result}
    except ValueError as e:
        return {"current_step": "error", "error": str(e)}

def node_call_llm(state: GraphState) -> dict:
    \"\"\"Chama LLM com tratamento de erro.\"\"\"
    from langchain_openai import ChatOpenAI
    
    llm = ChatOpenAI(
        model=config.llm_model,
        temperature=config.llm_temperature
    )
    
    response = llm.invoke(state["messages"])
    return {"messages": [response], "current_step": "llm_called"}

# Importar todos em __init__.py
__all__ = ["node_process_input", "node_call_llm"]
```

#### graph.py - Construção do Graph
```python
# my_agent/graph.py
from langgraph.graph import StateGraph, START, END
from my_agent.state import GraphState
from my_agent.config import config
from my_agent.utils.nodes import node_process_input, node_call_llm

def create_graph():
    \"\"\"Factory function para criar graph configurável.\"\"\"
    workflow = StateGraph(GraphState)
    
    # Adicionar nodes
    workflow.add_node("input", node_process_input)
    workflow.add_node("llm", node_call_llm)
    
    # Adicionar edges
    workflow.add_edge(START, "input")
    workflow.add_edge("input", "llm")
    workflow.add_edge("llm", END)
    
    return workflow.compile(checkpointer=config.checkpoint_config)

# Compilar na importação
graph = create_graph()
```

#### tests/ - Testes Estruturados
```python
# tests/test_graph.py
import pytest
from my_agent.graph import graph
from my_agent.state import GraphState

@pytest.fixture
def sample_state():
    return GraphState(
        messages=[{"role": "user", "content": "test"}],
        current_step="start"
    )

def test_graph_execution(sample_state):
    \"\"\"Testa execução completa do graph.\"\"\"
    config = {"configurable": {"thread_id": "test-1"}}
    result = graph.invoke(sample_state, config)
    
    assert "messages" in result
    assert result["current_step"] in ["error", "llm_called"]

def test_node_independently():
    \"\"\"Testa node isoladamente.\"\"\"
    from my_agent.utils.nodes import node_process_input
    
    state = GraphState(messages=[{"content": "valid input"}])
    result = node_process_input(state)
    
    assert result["current_step"] == "processed"
```

### 0.5.5 Imports e Organização

**✅ CORRETO - Imports explícitos e organizados:**
```python
# my_agent/graph.py
# Stdlib
import os
from typing import Literal

# Third-party
from langgraph.graph import StateGraph, START, END
from langchain_openai import ChatOpenAI

# Local
from my_agent.state import GraphState
from my_agent.config import config
from my_agent.utils.nodes import node_a, node_b
from my_agent.utils.tools import tool_1, tool_2
```

**❌ ERRADO - Imports desorganizados:**
```python
from langgraph.graph import *
import my_agent.utils.nodes
from my_agent import *
from my_agent.utils.nodes import node_a as nA
```

### 0.5.6 Naming Conventions

| Elemento | Padrão | Exemplo |
|----------|--------|---------|
| **Package** | lowercase_with_underscores | `my_agent`, `customer_support` |
| **Module** | lowercase_with_underscores | `state.py`, `nodes.py` |
| **Class** | PascalCase | `GraphState`, `AgentConfig` |
| **Function** | lowercase_with_underscores | `process_input()`, `call_llm()` |
| **Constant** | UPPERCASE_WITH_UNDERSCORES | `MAX_RETRIES = 3` |
| **Node Name** | lowercase_with_underscores | `"process_input"`, `"call_llm"` |
| **Conditional Name** | Retorna string | `"continue"`, `"end"` |

### 0.5.7 .gitignore Completo para LangGraph

```
# Ambiente
.env
.env.local
.env.*.local

# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
venv/
env/
ENV/
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Tests
.pytest_cache/
.coverage
htmlcov/

# LangGraph/LangSmith
.langgraph_cache/
langgraph_data/
*.db
*.sqlite

# IDE
.vscode/
.idea/
*.swp
*.swo
*~
.DS_Store

# Notebooks
.ipynb_checkpoints/

# Logs
logs/
*.log
```

### 0.5.8 Makefile para Automação

```makefile
.PHONY: help install dev test lint format clean deploy

help:
	@echo "Available commands:"
	@echo "  make install    - Install dependencies"
	@echo "  make dev        - Run development server"
	@echo "  make test       - Run tests"
	@echo "  make lint       - Run linters"
	@echo "  make format     - Format code"

install:
	uv pip install -e ".[dev]"

dev:
	langgraph dev

test:
	pytest tests/ -v --cov=my_agent

lint:
	ruff check my_agent tests
	mypy my_agent

format:
	black my_agent tests
	ruff check --fix my_agent tests

clean:
	find . -type d -name __pycache__ -exec rm -rf {} +
	find . -type f -name "*.pyc" -delete
	rm -rf .pytest_cache .coverage htmlcov

deploy:
	langgraph build -t my-agent-image
```

### 0.5.9 Variáveis de Ambiente (.env.example)

```
# LLM Configuration
LLM_MODEL=gpt-4o
LLM_TEMPERATURE=0.7
OPENAI_API_KEY=sk-...

# Database
DB_CONNECTION=postgresql://user:pass@localhost:5432/langgraph

# Environment
ENV=development
DEBUG=true

# LangSmith (opcional)
LANGSMITH_API_KEY=
LANGSMITH_PROJECT=my-project

# Custom
MAX_ITERATIONS=10
TIMEOUT_SECONDS=300
```

### 0.5.10 Checklist de Estrutura de Projeto

Ao criar novo projeto LangGraph:

- [ ] Criar estrutura base com `langgraph new` ou manualmente
- [ ] Separar concerns: state, nodes, tools, config
- [ ] Usar TypedDict ou Pydantic para state (escolher um)
- [ ] Centralizar config em `config.py`
- [ ] Criar `__init__.py` para cada pacote
- [ ] Adicionar tests/ com estrutura similar
- [ ] Criar `.env.example` (COMMIT)
- [ ] Configurar `.gitignore`
- [ ] Adicionar `pyproject.toml` ou `requirements.txt`
- [ ] Criar `langgraph.json` para deployment
- [ ] Documentar em `README.md`
- [ ] Adicionar `Makefile` para automação
- [ ] Configurar IDE (VSCode settings.json, etc)

---

## SEÇÃO 1: Core Concepts & Architecture

### 1.1 State Management Avançado

#### State Schema com Validação
```python
from typing_extensions import TypedDict, Annotated
from pydantic import BaseModel, Field, validator
from langgraph.graph.message import add_messages
from operator import add

# Opção 1: TypedDict com Reducers (Performance)
class CoreState(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]
    metadata: Annotated[dict, lambda x, y: {**x, **y}]
    
# Opção 2: Pydantic para validação (Produção)
class ValidationSchema(BaseModel):
    user_id: str = Field(..., min_length=1, max_length=255)
    request_id: str = Field(default_factory=uuid4)
    retry_count: int = Field(default=0, ge=0, le=5)
    
    @validator("user_id")
    def validate_user_id(cls, v):
        if not re.match(r"^[a-zA-Z0-9_-]+$", v):
            raise ValueError("Invalid user_id format")
        return v
```

#### Advanced Reducers
```python
def smart_merge(left: dict, right: dict) -> dict:
    \"\"\"Merge que detecta conflitos e timestamps.\"\"\"
    result = left.copy()
    for key, value in right.items():
        if key in result and isinstance(value, dict):
            result[key] = smart_merge(result[key], value)
        else:
            result[key] = value
    return result

class State(TypedDict):
    results: Annotated[dict, smart_merge]
    execution_log: Annotated[list, add]
```

### 1.2 Node Implementation Patterns (Graph API)

#### Pure Functions with Dependency Injection
```python
from langgraph.runtime import Runtime
from dataclasses import dataclass

@dataclass
class AppContext:
    db_client: DatabaseClient
    http_client: httpx.AsyncClient
    user_id: str

def node_with_context(
    state: State, 
    runtime: Runtime[AppContext]
) -> dict:
    db = runtime.context.db_client
    user_id = runtime.context.user_id
    
    try:
        result = db.query(user_id)
        return {"query_result": result, "status": "success"}
    except DatabaseError as e:
        return {"error": str(e), "status": "failed"}
```

### 1.3 Conditional Edges & Routing

#### tools_condition Pattern
```python
from langgraph.prebuilt import ToolNode, tools_condition

workflow.add_conditional_edges(
    "agent",
    tools_condition,  # Automático
    {"tools": "tools", END: END}
)
```

#### Handoff Pattern
```python
from langgraph.types import Command
from typing import Literal

def agent_with_handoff(state: State) -> Command[Literal["supervisor", "specialist_a"]]:
    task_type = classify_task(state["input"])
    
    if task_type == "complex":
        return Command(
            goto="specialist_a",
            update={"task_routed_to": "specialist_a"}
        )
    else:
        return Command(goto="supervisor", update={"needs_review": True})
```

### 1.4 Functional API Patterns

#### Básico: Entrypoint + Task

```python
from langgraph.func import entrypoint, task
from langgraph.checkpoint.memory import MemorySaver

@task
def fetch_data(query: str) -> dict:
    \"\"\"Task assíncrona com checkpointing automático.\"\"\"
    result = external_api_call(query)
    return result

@entrypoint(checkpointer=MemorySaver())
def my_workflow(input_data: dict) -> dict:
    \"\"\"Entrypoint: ponto de entrada do workflow.\"\"\"
    
    # Task retorna Future-like - resolve com .result()
    data = fetch_data(input_data["query"]).result()
    
    # Processar com controle padrão Python
    if data:
        processed = process(data)
    else:
        processed = None
    
    return {"result": processed}

# Invocar
config = {"configurable": {"thread_id": "thread-1"}}
result = my_workflow.invoke({"query": "test"}, config)
```

#### Human-in-the-Loop com Functional API

```python
from langgraph.types import interrupt

@task
def generate_report(data: dict) -> str:
    \"\"\"Gerar relatório (checkpointed).\"\"\"
    return f"Report: {data}"

@entrypoint(checkpointer=checkpointer)
def approval_workflow(data: dict) -> dict:
    \"\"\"Workflow com aprovação humana.\"\"\"
    
    # Gerar relatório (checkpointed)
    report = generate_report(data).result()
    
    # Pausar para aprovação
    approval = interrupt({
        "report": report,
        "action": "Please approve or reject"
    })
    
    if approval:
        return {"status": "approved", "report": report}
    else:
        return {"status": "rejected"}
```

#### Short-term Memory com previous
```python
@entrypoint(checkpointer=checkpointer)
def counter_workflow(
    number: int, 
    *,
    previous: Any = None
) -> int:
    \"\"\"Memória automática com 'previous'.\"\"\"
    previous_sum = previous or 0
    return number + previous_sum

# Uso
config = {"configurable": {"thread_id": "counter-1"}}
counter_workflow.invoke(1, config)  # 1
counter_workflow.invoke(2, config)  # 3
counter_workflow.invoke(5, config)  # 8
```

#### Idempotency Pattern

```python
@task
def create_resource(resource_id: str) -> dict:
    \"\"\"Task idempotente - pode ser re-executada.\"\"\"
    
    # Check se já existe
    if resource_exists(resource_id):
        return get_resource(resource_id)
    
    # Criar com idempotency key
    return api.create(
        resource_id,
        idempotency_key=resource_id
    )
```

#### Class Methods com @task

```python
from langgraph.func import task

class DataProcessor:
    def __init__(self, config: dict):
        self.config = config
        # Registrar método como task
        self.process_task = task(self.process_data, name="process")
    
    def process_data(self, data: dict) -> dict:
        \"\"\"Este método será checkpointed.\"\"\"
        return {**data, "processed": True}

# Usar em entrypoint
processor = DataProcessor({})

@entrypoint(checkpointer=checkpointer)
def workflow(data: dict) -> dict:
    # Método de classe como task
    result = processor.process_task(data).result()
    return result
```

#### Long-term Memory com Store

```python
from langgraph.func import entrypoint, task
from langgraph.store import InMemoryStore

store = InMemoryStore()

@task
def save_memory(user_id: str, key: str, value: dict, *, store):
    \"\"\"Salvar em store long-term.\"\"\"
    store.put(f"user:{user_id}", key, value)
    return "saved"

@entrypoint(checkpointer=checkpointer, store=store)
def memory_workflow(user_id: str, fact: str) -> dict:
    \"\"\"Workflow com memória persistente.\"\"\"
    
    # Recuperar memória anterior
    memory = store.get(f"user:{user_id}", "facts") or []
    
    # Adicionar novo fato
    memory.append(fact)
    
    # Salvar
    save_memory(user_id, "facts", memory).result()
    
    return {"facts": memory}
```

---

## SEÇÃO 2: Tool Integration & Validation

### 2.1 Schema Validation com Pydantic
```python
from pydantic import BaseModel, Field, validator
from langchain_core.tools import tool

class SearchQuerySchema(BaseModel):
    query: str = Field(..., min_length=1, max_length=500)
    max_results: int = Field(default=5, ge=1, le=20)
    search_type: str = Field(default="web", pattern="^(web|news|academic)$")
    
    @validator("query")
    def query_not_empty_after_strip(cls, v):
        if not v.strip():
            raise ValueError("Query cannot be empty")
        return v.strip()

class SearchResult(BaseModel):
    title: str
    url: str
    snippet: str
    relevance_score: float = Field(ge=0, le=1)

@tool(args_schema=SearchQuerySchema)
def search_web(query: str, max_results: int = 5, search_type: str = "web") -> list[SearchResult]:
    \"\"\"Busca web com schema validation.\"\"\"
    try:
        results = perform_search(query, max_results, search_type)
        validated = [SearchResult(**r) for r in results]
        return validated
    except ValidationError as e:
        raise ValueError(f"Invalid search result format: {e}")
    except SearchError as e:
        raise RuntimeError(f"Search failed: {e}")
```

### 2.2 Dynamic Tool Selection
```python
def configure_tools_by_capability(state: State) -> list:
    \"\"\"Selecionar tools dinamicamente.\"\"\"
    all_tools = {
        "search": search_web,
        "math": calculator,
        "weather": get_weather,
    }
    
    requested_capabilities = state.get("required_capabilities", [])
    selected_tools = [all_tools[cap] for cap in requested_capabilities if cap in all_tools]
    
    user_id = state.get("user_id")
    allowed_tools = get_user_permissions(user_id)
    
    return [t for t in selected_tools if t.__name__ in allowed_tools]
```

---

## SEÇÃO 3: Persistence & Checkpointing

### 3.1 Durable Execution

**Definição:**
Execução durável que salva progresso em pontos-chave, permitindo pausar e retomar **exatamente** onde parou - mesmo após falhas ou delays prolongados (dias, semanas).

**Como funciona:**
1. Cada step do workflow é salvo no checkpointer
2. Se falhar, ao resumir: carrega estado do checkpoint
3. Continua de onde parou - sem reprocessar
4. Trabalho completado nunca é reexecutado

**Automático com:**
- **Graph API:** Checkpoint após cada superstep
- **Functional API:** Tasks e entrypoints checkpointed automaticamente

**Exemplo Real:**
```python
# Dia 1: Iniciar workflow
config = {"configurable": {"thread_id": "long-task-1"}}
result = graph.invoke(input_data, config)  # Pausa no HITL

# 1 semana depois: Resume sem reprocessar
result = graph.invoke(Command(resume="approved"), config)
# Passos anteriores: carregam resultados do checkpoint
```

**Design para Durable Execution:**
1. Determinismo: Mesma sequência de steps ao resumir
2. Idempotência: Safe para re-executar tasks se falhar
3. Encapsulação: Side effects em tasks (Functional API)
4. Checkpointer: Configurado apropriadamente

### 3.2 Checkpointer Selection

```python
# DESENVOLVIMENTO: Memory
from langgraph.checkpoint.memory import MemorySaver
checkpointer = MemorySaver()

# PRODUÇÃO PEQUENA: SQLite
from langgraph.checkpoint.sqlite import SqliteSaver
checkpointer = SqliteSaver.from_conn_string("sqlite:///graph_state.db")

# PRODUÇÃO GRANDE: PostgreSQL (recomendado)
from langgraph.checkpoint.postgres import PostgresSaver
checkpointer = PostgresSaver.from_conn_string(
    "postgresql://user:pass@localhost:5432/langgraph"
)

# Performance (ops/sec):
# MemorySaver: 8,392 | SQLite: 7,083 | PostgreSQL: 1,038
```

### 3.3 Store com Semantic Search

```python
from langgraph.store.memory import InMemoryStore

def embed(texts: list[str]) -> list[list[float]]:
    \"\"\"Use embedding model real.\"\"\"
    return embeddings_model.embed_documents(texts)

# Store com semantic search
store = InMemoryStore(
    index={"embed": embed, "dims": 384}
)

# Namespace hierarchical
namespace = ("user-123", "preferences")

# Put com conteúdo semanticamente searchable
store.put(namespace, "diet", {
    "restrictions": ["vegetarian", "gluten-free"],
    "preferences": ["italian", "thai"]
})

# Semantic search
results = store.search(
    namespace,
    query="food restrictions",
    filter={"type": "diet"},  # Content filter
    limit=5
)

# Cross-namespace search
all_results = store.search(
    ("user-123",),  # Base namespace
    query="dietary preferences",
    recursive=True  # Search all children
)
```

---

## SEÇÃO 4: Advanced Patterns

### 4.1 Multi-Agent Orchestration

#### Supervisor Pattern
```python
def create_supervisor_agent(agents: dict[str, str]):
    \"\"\"Criar supervisor que roteia para especialistas.\"\"\"
    
    supervisor_llm = ChatOpenAI(model="gpt-4").bind_tools(
        [agent_tool for agent_tool in agents.values()]
    )
    
    def supervisor_node(state: MessagesState) -> Command:
        response = supervisor_llm.invoke(state["messages"])
        
        if response.tool_calls:
            agent_to_call = response.tool_calls[0]["name"]
            return Command(
                goto=agent_to_call,
                update={"messages": [response]}
            )
        else:
            return Command(goto=END, update={"messages": [response]})
    
    return supervisor_node
```

#### Parallel Execution com Send()
```python
from langgraph.types import Send

def distribute_tasks(state: State):
    \"\"\"Fan-out paralelo.\"\"\"
    return [
        Send("worker_agent", {"task": task, "task_id": i})
        for i, task in enumerate(state["tasks"])
    ]

def aggregate_results(state: State) -> dict:
    \"\"\"Fan-in agregação.\"\"\"
    all_results = [v for k, v in state.items() if k.startswith("worker_result_")]
    return {"final_results": all_results}
```

### 4.2 Human-in-the-Loop

#### Dynamic Interrupts
```python
from langgraph.types import interrupt

def approval_required_node(state: State) -> dict:
    \"\"\"Pausar para aprovação humana.\"\"\"
    
    approval_context = {
        "action": "high_risk_transaction",
        "amount": state["transaction_amount"],
        "timestamp": datetime.now().isoformat()
    }
    
    human_input = interrupt(
        f"Approval required for ${state['transaction_amount']}"
    )
    
    if human_input == "approve":
        return {"approved": True}
    else:
        return {"approved": False}
```

### 4.3 Streaming & Custom Events

#### Multi-mode Streaming
```python
async def stream_modes(graph, input_state, config):
    \"\"\"Diferentes stream modes.\"\"\"
    
    # Values - estado completo
    async for chunk in graph.astream(input_state, config, stream_mode="values"):
        print(f"Full state: {chunk}")
    
    # Updates - apenas mudanças
    async for chunk in graph.astream(input_state, config, stream_mode="updates"):
        print(f"Updates: {chunk}")
    
    # Messages - tokens LLM
    async for chunk in graph.astream(input_state, config, stream_mode="messages"):
        print(f"Message: {chunk}")
```

#### Custom Events com writer

```python
def node_with_custom_events(state: State, config):
    \"\"\"Emit custom events durante processamento.\"\"\"
    
    writer = config.get("writer", lambda x: None)
    items = state["items"]
    
    for i, item in enumerate(items):
        result = process_item(item)
        
        # Emit progress
        writer({
            "type": "progress",
            "current": i + 1,
            "total": len(items),
            "percentage": ((i + 1) / len(items)) * 100
        })
    
    return {"processed_items": items}

# Stream com custom events
async for chunk in graph.astream(
    input_data,
    config,
    stream_mode=["custom", "updates"]
):
    event_type, data = chunk
    if event_type == "custom":
        print(f"Progress: {data}")
```

### 4.4 Agent Jumps em Middleware

```python
from langchain.agents.middleware import AgentMiddleware, hook_config

class ConditionalJumpMiddleware(AgentMiddleware):
    @hook_config(can_jump_to=["end", "tools"])
    def after_model(self, state, runtime):
        \"\"\"Pular para node específico baseado em condição.\"\"\"
        
        if should_end(state):
            return {"jump_to": "end"}
        
        if should_skip_tools(state):
            return {"jump_to": "model"}
        
        return None

# Targets disponíveis: "end", "tools", "model"
```

---

## SEÇÃO 5: Production Optimization

### 5.1 Performance Benchmarks

```python
CHECKPOINTER_PERFORMANCE = {
    "MemorySaver": "8,392 ops/sec",      # Dev
    "SQLite": "7,083 ops/sec",           # Local
    "Redis": "2,950 ops/sec",            # Cache
    "MySQL": "1,152 ops/sec",            # Prod
    "PostgreSQL": "1,038 ops/sec"        # Enterprise
}

# Recomendações:
# Dev: MemorySaver
# Small prod: SQLite or Redis
# Large prod: PostgreSQL + Redis
```

### 5.2 Memory Optimization

```python
# 1. __slots__ em classes
class OptimizedContext:
    __slots__ = ["user_id", "request_id", "timestamp"]

# 2. Evitar cópias desnecessárias
def efficient_update(state: State) -> dict:
    existing_data = state.get("data", {})
    existing_data["new_field"] = "value"
    return {"data": existing_data}

# 3. Message trimming
def trim_messages(state: State, max_tokens: int = 4096) -> dict:
    messages = state["messages"]
    system_messages = [m for m in messages if isinstance(m, SystemMessage)]
    other_messages = [m for m in messages if not isinstance(m, SystemMessage)]
    
    total_tokens = count_tokens(messages)
    
    if total_tokens > max_tokens:
        recent_messages = other_messages[-10:]
        return {
            "messages": system_messages + recent_messages,
            "trimmed": True
        }
    
    return {"messages": messages, "trimmed": False}
```

### 5.3 Async Best Practices

```python
# ✅ Correto
async def async_io_node(state: State) -> dict:
    \"\"\"Operações IO-bound paralelas.\"\"\"
    
    results = await asyncio.gather(
        fetch_api_1(),
        fetch_api_2(),
        fetch_api_3(),
        return_exceptions=True
    )
    
    validated = [r for r in results if not isinstance(r, Exception)]
    return {"api_results": validated}

# Invocação correta
result = graph.ainvoke(input_data, config)
```

---

## SEÇÃO 6: Error Handling & Reliability

### 6.1 Error Categorization

```python
class ErrorCategory:
    VALIDATION = "validation"
    TEMPORARY = "temporary"      # Retry
    PERMANENT = "permanent"      # Não retry
    RATE_LIMIT = "rate_limit"    # Backoff exponencial
    AUTHENTICATION = "auth"      # Revalidar

def categorize_error(error: Exception) -> str:
    \"\"\"Categorizar para estratégia apropriada.\"\"\"
    if isinstance(error, ValidationError):
        return ErrorCategory.VALIDATION
    elif isinstance(error, (TimeoutError, ConnectionError)):
        return ErrorCategory.TEMPORARY
    elif isinstance(error, RateLimitError):
        return ErrorCategory.RATE_LIMIT
    else:
        return ErrorCategory.PERMANENT

def error_handling_node(state: State) -> dict:
    \"\"\"Tratamento de erros com retry.\"\"\"
    
    error = state.get("error")
    if not error:
        return {"status": "no_error"}
    
    category = categorize_error(error)
    retry_count = state.get("retry_count", 0)
    
    if category == ErrorCategory.TEMPORARY and retry_count < 3:
        delay = min(2 ** retry_count, 30)
        return {
            "status": "retrying",
            "retry_count": retry_count + 1,
            "retry_delay": delay
        }
    
    elif category == ErrorCategory.RATE_LIMIT:
        delay = (2 ** retry_count) + random.uniform(0, 1)
        return {
            "status": "rate_limited",
            "retry_delay": delay,
            "retry_count": retry_count + 1
        }
    
    else:
        return {
            "status": "failed",
            "error_category": category,
            "error_message": str(error)
        }
```

### 6.2 Retry with Exponential Backoff

```python
import time
import random

def retry_with_backoff(
    func,
    max_retries: int = 3,
    base_delay: float = 1.0,
    max_delay: float = 60.0,
    jitter: bool = True
):
    \"\"\"Decorator para retry com backoff.\"\"\"
    
    def wrapper(*args, **kwargs):
        for attempt in range(max_retries):
            try:
                return func(*args, **kwargs)
            except Exception as e:
                if attempt == max_retries - 1:
                    raise
                
                delay = min(base_delay * (2 ** attempt), max_delay)
                
                if jitter:
                    delay = delay * (0.5 + random.random())
                
                logger.warning(
                    f"Attempt {attempt + 1} failed, retrying in {delay:.2f}s",
                    exc_info=e
                )
                time.sleep(delay)
    
    return wrapper

@retry_with_backoff(max_retries=3)
def unreliable_operation():
    return external_api_call()
```

---

## SEÇÃO 7: Debugging & Observability

### 7.1 LangGraph Studio Integration

```python
# langgraph.json
{
    "graphs": {
        "main": {
            "path": "src/graph.py:create_graph",
            "name": "Main Agent"
        }
    },
    "env": ".env"
}

# Executar
# langgraph up --port 2024
```

### 7.2 LangSmith & Langfuse

```python
from langsmith import traceable
from langfuse.langchain import CallbackHandler

@traceable
def traced_operation(state: State):
    return process(state)

langfuse_handler = CallbackHandler()

graph = (
    graph_builder.compile()
    .with_config({
        "callbacks": [langfuse_handler],
        "tags": ["production"]
    })
)
```

### 7.3 Custom Debugging

```python
def debug_node(state: State) -> dict:
    logger.info(f\"\"\"
    === Debug Info ===
    Timestamp: {datetime.now().isoformat()}
    State Keys: {list(state.keys())}
    Message Count: {len(state.get('messages', []))}
    Current Step: {state.get('current_step')}
    \"\"\")
    
    import pdb; pdb.set_trace()
    return state
```

### 7.4 LangSmith Deployment

**LangSmith Deployment** (1-click deploy):

**Pré-requisitos:**
1. ✅ Código em GitHub (público ou privado)
2. ✅ Conta LangSmith ativa
3. ✅ App testado localmente

**Steps:**
1. Push para GitHub
2. LangSmith → Deployments → "New"
3. Conectar GitHub
4. Selecionar repositório
5. Deploy automático (~15 min)

**Features:**
- ✅ APIs robustas para checkpointing
- ✅ Memory management automático
- ✅ Conversation threads
- ✅ Horizontal scaling
- ✅ Rollback instantâneo
- ✅ Studio integration

---

## SEÇÃO 8: Common Pitfalls & Anti-Patterns

### ❌ Pitfall 1: State Mutation
```python
# ERRADO
def bad_node(state: State):
    state["messages"].append(new_message)  # MUTAÇÃO
    return state

# CORRETO
def good_node(state: State) -> dict:
    return {"messages": [*state["messages"], new_message]}
```

### ❌ Pitfall 2: Complex Routing Logic
```python
# ERRADO - Logic pesada no router
def bad_router(state: State) -> str:
    analysis = expensive_ml_model(state["input"])  # ERRADO
    return decision

# CORRETO - Router simples
def good_router(state: State) -> str:
    return "path_a" if state.get("analysis_result", 0) > threshold else "path_b"
```

### ❌ Pitfall 3: Inconsistent Error Handling
```python
# ERRADO
def bad_handling(state: State):
    try:
        result1 = operation1()
        result2 = operation2()  # Se falhar, ninguém sabe
        return {"results": [result1, result2]}
    except:
        pass
    return {}

# CORRETO
def good_handling(state: State) -> dict:
    results = []
    errors = []
    
    for i, op in enumerate([operation1, operation2]):
        try:
            results.append(op())
        except Exception as e:
            errors.append({"op": i, "error": str(e)})
    
    return {"results": results, "errors": errors}
```

### ❌ Pitfall 4: Async/Sync Mixing
```python
# ERRADO
def bad_async(state: State):
    result = asyncio.run(async_op())  # ERRADO
    return {"result": result}

# CORRETO
async def good_async(state: State):
    result = await async_op()
    return {"result": result}

# Invocação: graph.ainvoke()
```

### ❌ Pitfall 5: Non-Deterministic Side Effects
```python
# ERRADO - Side effect fora de task
@entrypoint(checkpointer=checkpointer)
def bad_workflow(data: dict) -> dict:
    send_email(data["email"])  # Executado 2x ao resumir!
    approval = interrupt("Approve?")
    return {"approved": approval}

# CORRETO - Side effect em task
@task
def send_email_task(email: str) -> str:
    send_email(email)
    return "sent"

@entrypoint(checkpointer=checkpointer)
def good_workflow(data: dict) -> dict:
    result = send_email_task(data["email"]).result()
    approval = interrupt("Approve?")
    return {"approved": approval, "email_sent": result}
```

---

## SEÇÃO 9: Real-World Implementation Patterns

### 9.1 Web Search Agent (Graph API - Tavily)
```python
from langchain_tavily import TavilySearchResults
from langchain_core.tools import tool

tavily_search = TavilySearchResults(max_results=3)

@tool
def search_web(query: str) -> str:
    \"\"\"Busca na web usando Tavily.\"\"\"
    results = tavily_search.invoke(query)
    return format_search_results(results)

llm = ChatOpenAI(model="gpt-4")
tools = [search_web]

from langgraph.prebuilt import create_react_agent  # Deprecated
# Use create_agent em vez disso (veja seção 11)
```

### 9.2 Multi-Agent Supervisor (Graph API)
```python
def create_multi_agent_system():
    \"\"\"Sistema multi-agente.\"\"\"
    
    research_agent = create_research_agent()
    analysis_agent = create_analysis_agent()
    writing_agent = create_writing_agent()
    
    def supervisor_node(state: MessagesState) -> Command:
        response = supervisor_llm.invoke(state["messages"])
        next_agent = response.tool_calls[0]["name"]
        return Command(goto=next_agent, update={"messages": [response]})
    
    builder = StateGraph(MessagesState)
    builder.add_node("supervisor", supervisor_node)
    builder.add_node("research", research_agent)
    builder.add_node("analysis", analysis_agent)
    builder.add_node("writing", writing_agent)
    
    builder.add_edge(START, "supervisor")
    builder.add_edge("research", "supervisor")
    builder.add_edge("analysis", "supervisor")
    builder.add_edge("writing", "supervisor")
    builder.add_edge("supervisor", END)
    
    return builder.compile()
```

### 9.3 Web Search Agent (Functional API)
```python
from langgraph.func import entrypoint, task
from duckduckgo_search import DDGS

@task
def web_search(query: str, max_results: int = 5) -> str:
    \"\"\"Busca web checkpointed.\"\"\"
    with DDGS() as ddgs:
        results = list(ddgs.text(query, max_results=max_results))
    return format_results(results)

@task
def llm_response(query: str, context: str) -> str:
    \"\"\"Resposta LLM.\"\"\"
    from langchain_openai import ChatOpenAI
    llm = ChatOpenAI(model="gpt-4")
    prompt = f"Context: {context}\n\nQuestion: {query}"
    return llm.invoke(prompt).content

@entrypoint(checkpointer=MemorySaver())
def search_agent(user_query: str, *, previous: list = None) -> dict:
    \"\"\"Agent com busca web e memória.\"\"\"
    
    history = previous or []
    
    # Busca (checkpointed)
    search_results = web_search(user_query).result()
    
    # Resposta (checkpointed)
    response = llm_response(user_query, search_results).result()
    
    # Histórico
    new_history = history + [{"query": user_query, "response": response}]
    
    return {"response": response, "history": new_history}
```

### 9.4 Customer Support Bot com create_agent

```python
from langchain.agents import create_agent
from langchain.agents.middleware import (
    SummarizationMiddleware,
    PIIMiddleware,
    HumanInTheLoopMiddleware,
    ModelCallLimitMiddleware
)
from langgraph.checkpoint.postgres import PostgresSaver

agent = create_agent(
    model="gpt-4o",
    tools=[search_kb, create_ticket, send_email],
    system_prompt="You are a helpful customer support agent",
    checkpointer=PostgresSaver.from_conn_string("postgresql://..."),
    middleware=[
        # Detectar/redactar emails e CPF
        PIIMiddleware("email", strategy="mask", apply_to_input=True),
        PIIMiddleware("cpf", strategy="redact", apply_to_input=True),
        
        # Resumir conversas longas
        SummarizationMiddleware(
            model="gpt-4o-mini",
            max_tokens_before_summary=3000,
            messages_to_keep=20
        ),
        
        # Aprovação humana para emails
        HumanInTheLoopMiddleware(
            interrupt_on={"send_email": True}
        ),
        
        # Limitar chamadas ao modelo
        ModelCallLimitMiddleware(
            thread_limit=100,
            run_limit=20
        )
    ]
)

# Usar
config = {"configurable": {"thread_id": "customer-123"}}
result = agent.invoke(
    {"messages": [HumanMessage(content="Help me with my order")]},
    config
)
```

---

## SEÇÃO 10: LangGraph Ecosystem Packages

### 10.1 langgraph-supervisor

```python
from langgraph_supervisor import create_supervisor_node

# Criar supervisor automaticamente
supervisor = create_supervisor_node(
    name="supervisor",
    agents={
        "research": research_agent,
        "analysis": analysis_agent,
        "writing": writing_agent
    },
    model="gpt-4o",
    system_prompt="Route tasks to appropriate agents"
)

builder.add_node("supervisor", supervisor)
```

### 10.2 langgraph-swarm

```python
from langgraph_swarm import create_swarm

# Multi-agent swarm
swarm = create_swarm(
    agents=[agent1, agent2, agent3],
    communication_pattern="broadcast"  # ou "mesh", "star"
)

result = swarm.invoke({"task": "..."})
```

### 10.3 langmem

```python
from langmem import ShortTermMemory, LongTermMemory, MemoryManager

manager = MemoryManager(
    short_term=ShortTermMemory(capacity=20),
    long_term=LongTermMemory(db="postgres://..."),
    user_id="user-123"
)

# Salvar memória
manager.save("preference", {"favorite_color": "blue"})

# Recuperar
preference = manager.recall("preference")
```

### 10.4 agentevals

```python
from agentevals import evaluate_agent, MetricsCollector

collector = MetricsCollector()

# Evaluar performance
results = evaluate_agent(
    agent=my_agent,
    test_cases=[
        {"input": "...", "expected": "..."},
        {"input": "...", "expected": "..."}
    ],
    metrics=["accuracy", "latency", "cost"],
    collector=collector
)

print(f"Accuracy: {results.accuracy}")
```

---

## SEÇÃO 11: LangChain v1 create_agent - High-Level API

### 11.1 Visão Geral

**create_agent** é o novo API de alto nível do LangChain v1 que substitui `create_react_agent` (DEPRECATED).

**Diferencial:** Built on LangGraph + Middleware system = máxima flexibilidade

**Use quando:**
- ✅ Quer começar rápido (high-level)
- ✅ Precisa de middleware (PII, HITL, summarization)
- ✅ Não precisa customizar orchestration profundamente
- ✅ Quer durable execution automático

### 11.2 Básico

```python
from langchain.agents import create_agent
from langchain_openai import ChatOpenAI
from langchain.messages import HumanMessage

# Criar agent simples
agent = create_agent(
    model="gpt-4o",  # String ou ChatOpenAI instance
    tools=[search_tool, calculator_tool],
    system_prompt="You are a helpful assistant"
)

# Invocar
result = agent.invoke({
    "messages": [HumanMessage(content="What is 2+2?")]
})

print(result["messages"][-1].content)
```

### 11.3 Com Checkpointing (Durable Execution)

```python
from langgraph.checkpoint.memory import MemorySaver
from langchain.agents import create_agent

agent = create_agent(
    model="gpt-4o",
    tools=[search_web, send_email],
    checkpointer=MemorySaver(),  # Necessário para persistence
    system_prompt="Customer support agent"
)

# Primeiro invoke
config = {"configurable": {"thread_id": "customer-123"}}
result = agent.invoke(
    {"messages": [HumanMessage(content="Help!")]},
    config
)

# Semana depois - resuma onde parou
from langgraph.types import Command
result = agent.invoke(
    Command(resume="approved"),
    config
)
```

### 11.4 Migration de create_react_agent

```python
# ❌ DEPRECATED (v0)
from langgraph.prebuilt import create_react_agent

agent = create_react_agent(
    model=ChatOpenAI(model="gpt-4o"),
    tools=tools
)

# ✅ NOVO (v1)
from langchain.agents import create_agent

agent = create_agent(
    model="gpt-4o",  # String ou modelo instance
    tools=tools,
    checkpointer=PostgresSaver(...)  # Optional mas recomendado
)
```

---

## SEÇÃO 12: Middleware System

### 12.1 O que é Middleware?

Middleware permite injetar lógica em pontos específicos do agente:
- **Before model call**: Validar input, filtrar tools, modificar prompts
- **After model call**: Validar output, aplicar guardrails
- **Around tool call**: Retry, logging, error handling

### 12.2 Prebuilt Middleware (12 Tipos)

#### PIIMiddleware - Detectar/Redactar PII

```python
from langchain.agents.middleware import PIIMiddleware

agent = create_agent(
    model="gpt-4o",
    tools=tools,
    middleware=[
        # Redact emails
        PIIMiddleware("email", strategy="redact", apply_to_input=True),
        
        # Mask credit cards
        PIIMiddleware("credit_card", strategy="mask", apply_to_input=True),
        
        # Custom PII com regex
        PIIMiddleware(
            "api_key",
            detector=r"sk-[a-zA-Z0-9]{32}",
            strategy="block"  # Raise error
        )
    ]
)

# Strategies: "redact", "mask", "hash", "block"
```

#### HumanInTheLoopMiddleware - Aprovação Humana

```python
from langchain.agents.middleware import HumanInTheLoopMiddleware

agent = create_agent(
    model="gpt-4o",
    tools=tools,
    middleware=[
        HumanInTheLoopMiddleware(
            interrupt_on={
                "send_email": {
                    "allowed_decisions": ["approve", "edit", "reject"]
                },
                "delete_data": True,  # Always interrupt
                "read_file": False    # Auto-approve
            }
        )
    ]
)
```

#### SummarizationMiddleware - Gerenciar Contexto

```python
from langchain.agents.middleware import SummarizationMiddleware

agent = create_agent(
    model="gpt-4o",
    tools=tools,
    middleware=[
        SummarizationMiddleware(
            model="gpt-4o-mini",
            max_tokens_before_summary=4000,
            messages_to_keep=20,
            summary_prompt="Summarize: {messages}"  # Optional
        )
    ]
)
```

#### ToolRetryMiddleware - Retry Automático

```python
from langchain.agents.middleware import ToolRetryMiddleware

agent = create_agent(
    model="gpt-4o",
    tools=tools,
    middleware=[
        ToolRetryMiddleware(
            max_retries=3,
            backoff_factor=2.0,
            initial_delay=1.0,
            max_delay=60.0,
            jitter=True
        )
    ]
)
```

#### ModelCallLimitMiddleware - Limitar Chamadas

```python
agent = create_agent(
    model="gpt-4o",
    tools=tools,
    middleware=[
        ModelCallLimitMiddleware(
            thread_limit=50,      # Max calls per thread
            run_limit=10,         # Max calls per run
            exit_behavior="end"   # "end" ou "error"
        )
    ]
)
```

#### ToolCallLimitMiddleware - Limitar Tool Calls

```python
from langchain.agents.middleware import ToolCallLimitMiddleware

agent = create_agent(
    model="gpt-4o",
    tools=tools,
    middleware=[
        ToolCallLimitMiddleware(
            max_tool_calls=5,
            exit_behavior="end"
        )
    ]
)
```

#### ModelFallbackMiddleware - Fallback Automático

```python
from langchain.agents.middleware import ModelFallbackMiddleware

agent = create_agent(
    model="gpt-4o",
    tools=tools,
    middleware=[
        ModelFallbackMiddleware(
            fallback_models=["gpt-4-turbo", "gpt-3.5-turbo"],
            retry_on_errors=[Exception]
        )
    ]
)
```

#### AnthropicPromptCachingMiddleware - Cache de Prompts

```python
from langchain.agents.middleware import AnthropicPromptCachingMiddleware

# Reduz custos com Claude
agent = create_agent(
    model="claude-opus",
    tools=tools,
    middleware=[
        AnthropicPromptCachingMiddleware(
            cache_threshold=1024  # Cache se > 1024 tokens
        )
    ]
)
```

#### Mais Middleware (8 tipos adicionais)

```python
from langchain.agents.middleware import (
    TodoListMiddleware,           # Planning
    LLMToolSelectorMiddleware,    # Dynamic selection
    LLMToolEmulatorMiddleware,    # Testing/emulation
    ContextEditingMiddleware,     # Gerenciar contexto
    LLMCallLoggingMiddleware,     # Logging
    ErrorHandlingMiddleware,      # Error handling
    RateLimitMiddleware,          # Rate limiting
    CostTrackingMiddleware        # Cost tracking
)
```

### 12.3 Custom Middleware - Decorator-based

```python
from langchain.agents.middleware import (
    before_model, after_model, wrap_model_call,
    before_agent, after_agent, wrap_tool_call, dynamic_prompt
)

# Logging antes do model
@before_model
def log_before(state, runtime):
    print(f"Calling model with {len(state['messages'])} messages")
    return None

# Validação após o model
@after_model(can_jump_to=["end"])
def validate_output(state, runtime):
    if "BLOCKED" in state["messages"][-1].content:
        return {
            "messages": [AIMessage("Cannot respond")],
            "jump_to": "end"
        }
    return None

# Retry logic com wrap
@wrap_model_call
def retry_model(request, handler):
    for attempt in range(3):
        try:
            return handler(request)
        except Exception as e:
            if attempt == 2:
                raise
            print(f"Retry {attempt + 1}/3")

# Modificar prompt dinamicamente
@dynamic_prompt
def inject_context(state, runtime):
    return {
        "system_prompt": state.get("system_prompt", "") + 
                        f"\nUser: {state.get('user_id', 'unknown')}"
    }

# Usar
agent = create_agent(
    model="gpt-4o",
    tools=tools,
    middleware=[log_before, validate_output, retry_model, inject_context]
)
```

### 12.4 Custom Middleware - Class-based

```python
from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse
from typing import Callable

class CustomRetryMiddleware(AgentMiddleware):
    def __init__(self, max_retries: int = 3):
        super().__init__()
        self.max_retries = max_retries
    
    def wrap_model_call(
        self,
        request: ModelRequest,
        handler: Callable[[ModelRequest], ModelResponse]
    ) -> ModelResponse:
        for attempt in range(self.max_retries):
            try:
                return handler(request)
            except Exception as e:
                if attempt == self.max_retries - 1:
                    raise
                print(f"Retry {attempt + 1}/{self.max_retries}: {e}")

# Usar
agent = create_agent(
    model="gpt-4o",
    tools=tools,
    middleware=[CustomRetryMiddleware(max_retries=5)]
)
```

### 12.5 Middleware Hooks

**Sequential Hooks:**
```python
# Executados em ordem
@before_agent
def setup(state, runtime): pass

@before_model
def prepare(state, runtime): pass

@after_model
def validate(state, runtime): pass

@after_agent
def cleanup(state, runtime): pass
```

**Wrapping Hooks:**
```python
# Interceptam a execução
@wrap_model_call
def around_model(request, handler): pass

@wrap_tool_call
def around_tool(tool_call, handler): pass
```

### 12.6 Agent Jumps - Pular para Nodes

```python
from langchain.agents.middleware import hook_config

class ConditionalJumpMiddleware(AgentMiddleware):
    @hook_config(can_jump_to=["end", "tools"])
    def after_model(self, state, runtime):
        \"\"\"Pular para node específico.\"\"\"
        
        if should_end(state):
            return {"jump_to": "end"}
        
        if should_skip_tools(state):
            return {"jump_to": "tools"}
        
        return None

# Targets: "end", "tools", "model"
```

---

## SEÇÃO 13: Template de Documento de Tarefa (v2.3)

### Estrutura Padrão

```markdown
# Documento de Tarefa: [FUNCIONALIDADE]

## 1. Análise Atual
### 1.1 Estado Atual
- Project Structure: [descrição]
- State Schema: [descrever]
- Nodes/Tasks: [listar]
- Middleware: [aplicado?]
- Performance: [benchmarks]
- API: Graph/Functional/create_agent?

### 1.2 Impacto da Mudança
- Componentes afetados
- Riscos identificados

## 2. Requisitos & Objetivos
### 2.1 Objetivo Principal
[Descrição clara]

### 2.2 Escolha de API
Graph API / Functional API / create_agent? Por quê?

### 2.3 Estrutura do Projeto
Qual estrutura usar? (Básica/Média/Avançada/Monorepo?)

## 3. Organização de Arquivos
### 3.1 Estrutura Base
[Estrutura específica do projeto]

### 3.2 Padrões de Importação
[Imports organizados conforme convenção]

## 4. Implementação Detalhada

### 4.1 State & Config
[state.py e config.py específicos]

### 4.2 Nodes/Tasks/Middleware
[Cada componente com responsabilidade, inputs/outputs, error handling]

### 4.3 Graph Construction
[graph.py com compilação]

## 5. Testing Strategy
### 5.1 Unit Tests
[Cada componente]

### 5.2 Integration Tests
[Fluxos completos]

## 6. Deployment & Monitoring
### 6.1 Estrutura Final
[Confirmação de estrutura]

### 6.2 Checklist
- [ ] Code review
- [ ] Tests passing
- [ ] Performance acceptable
- [ ] LangSmith Deployment ready
- [ ] Documentação completa
- [ ] .env.example criado
- [ ] langgraph.json configurado
```

---

## GUIA DE USO DO TEMPLATE v2.3

### Usar Este Template Para:

1. **Estrutura de Projeto:** Seção 0.5 (escolher estrutura apropriada)
2. **API Decision:** Seção 0 (Graph vs Functional vs create_agent)
3. **Graph API:** Seções 1-9
4. **Functional API:** Seções 1.4, 9.3, 4.3
5. **create_agent:** Seções 11, 12, 9.4
6. **Middleware:** Seção 12 completa
7. **Production:** Seções 5, 6, 7
8. **Packages:** Seção 10
9. **Organização de Código:** Seção 0.5 (estrutura + naming + imports)

### Checklist para Criação de Novo Projeto:

- [ ] Decidiu API apropriada (Graph/Functional/create_agent)
- [ ] Escolheu estrutura (Básica/Média/Avançada/Monorepo)
- [ ] Criou diretórios base
- [ ] Configurou pyproject.toml/requirements.txt
- [ ] Criou .env.example
- [ ] Criou langgraph.json
- [ ] Definiu State (state.py)
- [ ] Configurou AppConfig (config.py)
- [ ] Criou Nodes/Tasks (utils/nodes.py)
- [ ] Definiu Tools (utils/tools.py)
- [ ] Construiu Graph (graph.py)
- [ ] Criou testes (tests/)
- [ ] Configurou observability (LangSmith/Studio)
- [ ] Preparou deployment

### Checklist para Geração de Tarefas:

- [ ] Analisou base de código atual
- [ ] Escolheu API apropriada (Seção 0)
- [ ] Escolheu estrutura apropriada (Seção 0.5)
- [ ] Identificou scale requirements
- [ ] Escolheu checkpointer
- [ ] Configurou middleware (se create_agent)
- [ ] Implementou validation (Pydantic)
- [ ] Adicionou error handling
- [ ] Organizou código seguindo convenções
- [ ] Testou completamente
- [ ] Configurou observability
- [ ] Planejou rollback
- [ ] Documentou estrutura

---

**Template Version:** 2.3 Complete Production + Project Structure + Middleware System
**Last Updated:** 2025-11-01
**Compatible With:** LangGraph v1.0+, LangChain v1.0+
**Status:** Production Ready ✅
"""